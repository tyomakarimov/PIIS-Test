Приклад використання алгоритму QN у грі Space Invaders.

Основною функцією даного алгоритму є функція Q(s, a), яка повертає загальний результат
(стан лазера гравця, кількість очків), що матиме гравець після того, як зробить дію а в стані гри s.
Дій для гравця в даній грі може бути три: move вліво, move вправо та вистріл вверх по ворогах.
Q-value відповідає значенню стану середовища, у якому на даний момент часу перебуває гравець.
Також це значення враховує майбутні стани середовища, які на думку моделі, вона зможе досягти.
Q(s, a) для стану можна визначити шляхом обчислення винагороди, що спостерігається з середовища
в стані s за виконання дії a плюс знижене значення Q наступного стану, у якому опиниться гравець.
Тому вибирається наступна дія для гравця не по тому, яка дія в даний момент принесе найбільшу нагороду,
а та, яка максимізує виногороду враховуючи наступні стани.
Стан подається на вхід у вигляді картинки (поточного стану середовища з розташуванням усіх ворогів, щитів,
пуль і гравця). Далі для кожної з трьох можливих дій повертається нагорода.
Собираючи велику кількість інформації про те, після якого ходу, яка нагорода отримується,
нейронна мережа навчається. При чому навчання відбувається враховуючи майбутні стани гри та нагороди,
що будуть повернені в тих станах. Для тренування необхідно багато викликів функції. 
На 50 вже побачимо кращий результат, ніж якщо в просто рандомно гравець ходив.
На 200 викликах побачимо вже помітний результат.
